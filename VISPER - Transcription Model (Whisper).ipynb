{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xj3S4zBXKzIb"
      },
      "outputs": [],
      "source": [
        "# ------------------------ Install Required Packages ------------------------------\n",
        "\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q ffmpeg-python librosa numpy\n",
        "!sudo apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ TRANSCRIPTION MODEL (WHISPER AI) ------------------------------\n",
        "\n",
        "#----------------- IMPORTS ------------------\n",
        "import whisper\n",
        "import librosa\n",
        "import datetime\n",
        "import numpy as np\n",
        "from IPython.display import Audio\n",
        "from google.colab import files\n",
        "\n",
        "#----------------- FILE UPLOAD ------------------\n",
        "uploaded = files.upload()\n",
        "file_path = next(iter(uploaded))  # Get the uploaded file name\n",
        "Audio(file_path)  # Optional: play uploaded audio\n",
        "\n",
        "#----------------- WHISPER AI MODEL ------------------\n",
        "def transcribe_audio_with_context(file_path, model_size=\"base\"):\n",
        "    model = whisper.load_model(model_size)\n",
        "    result = model.transcribe(file_path, verbose=True)\n",
        "    return result[\"segments\"], result[\"text\"]\n",
        "\n",
        "#----------------- AUDIO ANALYSIS ------------------\n",
        "def analyze_audio(file_path):\n",
        "    y, sr = librosa.load(file_path)\n",
        "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "    tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
        "    return y, sr, beats\n",
        "\n",
        "#----------------- HELPER: TIMESTAMP FORMATTER ------------------\n",
        "def format_timestamp(seconds):\n",
        "    return str(datetime.timedelta(seconds=int(seconds)))\n",
        "\n",
        "#----------------- SEGMENT TAGGING WITH CONTEXT ------------------\n",
        "def tag_segments_with_context(segments, y, sr, beats):\n",
        "    tagged = []\n",
        "    beat_times = librosa.frames_to_time(beats, sr=sr)\n",
        "\n",
        "    for segment in segments:\n",
        "        start = segment['start']\n",
        "        end = segment['end']\n",
        "        text = segment['text'].strip()\n",
        "\n",
        "        segment_audio = y[int(start * sr):int(end * sr)]\n",
        "        energy = np.mean(np.square(segment_audio))\n",
        "        timestamp = f\"[{format_timestamp(start)} â†’ {format_timestamp(end)}]\"\n",
        "\n",
        "        # Check for background music or OST based on energy and beat alignment\n",
        "        if len(text.strip()) == 0:\n",
        "            if energy > 0.015 and any(abs(start - bt) < 1.0 for bt in beat_times):\n",
        "                tagged.append(f\"{timestamp} [Chorus]\")\n",
        "            elif energy > 0.01:\n",
        "                tagged.append(f\"{timestamp} [OST]\")\n",
        "            else:\n",
        "                continue  # skip low energy silences\n",
        "        else:\n",
        "            tagged.append(f\"{timestamp} {text}\")\n",
        "\n",
        "    return \"\\n\".join(tagged)\n",
        "\n",
        "#----------------- EXECUTION ------------------\n",
        "segments, _ = transcribe_audio_with_context(file_path)\n",
        "y, sr, beats = analyze_audio(file_path)\n",
        "final_transcription = tag_segments_with_context(segments, y, sr, beats)\n",
        "\n",
        "#----------------- OUTPUT ------------------\n",
        "print(\"\\n Final Transcription with Context:\\n\")\n",
        "print(final_transcription)\n"
      ],
      "metadata": {
        "id": "lRF5OfB4LHyG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}